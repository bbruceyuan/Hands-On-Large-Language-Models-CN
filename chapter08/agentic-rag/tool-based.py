"""
Agentic RAG MVP ç¤ºä¾‹
å®ç°ä¸€ä¸ªæœ€å°å¯ç”¨çš„ Agentic RAG ç³»ç»Ÿï¼Œæ¼”ç¤ºå¦‚ä½•é€šè¿‡å·¥å…·ç»„åˆå®ç°"å…ˆç²—åç»†"çš„è¯æ®æ”¶é›†ç­–ç•¥
"""

from typing import List, Dict
import json
from dataclasses import dataclass
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


@dataclass
class FileChunk:
    """æ–‡ä»¶ç‰‡æ®µ"""

    file_id: int
    chunk_index: int
    content: str


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""

    id: int
    filename: str
    chunk_count: int
    status: str = "done"


class MockKnowledgeBaseController:
    """æ¨¡æ‹ŸçŸ¥è¯†åº“æ§åˆ¶å™¨ - å†…å­˜ç‰ˆæœ¬ï¼Œç”¨äºæ¼”ç¤º"""

    def __init__(self):
        # æ¨¡æ‹Ÿä¸€äº›æ–‡æ¡£æ•°æ®
        self.files = [
            FileInfo(1, "rag_introduction.md", 5),
            FileInfo(2, "llm_fundamentals.md", 4),
            FileInfo(3, "vector_search.md", 3),
            FileInfo(4, "prompt_engineering.md", 4),
        ]

        # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹ç‰‡æ®µ
        self.chunks = {
            (1, 0): FileChunk(
                1,
                0,
                "RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚",
            ),
            (1, 1): FileChunk(
                1,
                1,
                "RAG çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š1) èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯ï¼Œ2) å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œ3) æä¾›å¯è¿½æº¯çš„ä¿¡æ¯æ¥æºï¼Œ4) æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†ã€‚",
            ),
            (1, 2): FileChunk(
                1,
                2,
                "RAG çš„ç¼ºç‚¹åŒ…æ‹¬ï¼š1) æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆæ•ˆæœï¼Œ2) å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦ï¼Œ3) å¯¹å‘é‡æ•°æ®åº“çš„ä¾èµ–ï¼Œ4) å¯èƒ½å­˜åœ¨æ£€ç´¢å»¶è¿Ÿã€‚",
            ),
            (1, 3): FileChunk(
                1,
                3,
                "ä¼ ç»Ÿ RAG ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ£€ç´¢-ç”Ÿæˆæµç¨‹ï¼Œæ— æ³•æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚",
            ),
            (1, 4): FileChunk(
                1,
                4,
                "Agentic RAG é€šè¿‡å¼•å…¥æ™ºèƒ½ä½“ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å†³ç­–ä½•æ—¶æ£€ç´¢ã€å¦‚ä½•æ£€ç´¢ä»¥åŠæ£€ç´¢å¤šå°‘å†…å®¹ï¼Œä»è€Œæå‡å¤æ‚é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚",
            ),
            (2, 0): FileChunk(
                2,
                0,
                "å¤§è¯­è¨€æ¨¡å‹ (LLM) æ˜¯åŸºäº Transformer æ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ã€‚",
            ),
            (2, 1): FileChunk(
                2, 1, "LLM çš„æ ¸å¿ƒèƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€ç”Ÿæˆã€æ¨ç†å’Œå°‘æ ·æœ¬å­¦ä¹ ç­‰ã€‚"
            ),
            (2, 2): FileChunk(
                2, 2, "LLM çš„å±€é™æ€§åŒ…æ‹¬çŸ¥è¯†æˆªæ­¢æ—¶é—´ã€å¯èƒ½äº§ç”Ÿå¹»è§‰ã€è®¡ç®—èµ„æºæ¶ˆè€—å¤§ç­‰ã€‚"
            ),
            (2, 3): FileChunk(
                2,
                3,
                "å·¥å…·è°ƒç”¨æ˜¯ LLM çš„é‡è¦æ‰©å±•èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ï¼Œæ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚",
            ),
            (3, 0): FileChunk(
                3,
                0,
                "å‘é‡æœç´¢æ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºæ¥å®ç°è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ã€‚",
            ),
            (3, 1): FileChunk(
                3,
                1,
                "å¸¸è§çš„å‘é‡æœç´¢ç®—æ³•åŒ…æ‹¬ FAISSã€Chromaã€Pinecone ç­‰ï¼Œå„æœ‰ä¸åŒçš„æ€§èƒ½ç‰¹ç‚¹ã€‚",
            ),
            (3, 2): FileChunk(
                3,
                2,
                "å‘é‡æœç´¢çš„æ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºembeddingæ¨¡å‹çš„è´¨é‡å’Œç´¢å¼•æ„å»ºç­–ç•¥ã€‚",
            ),
            (4, 0): FileChunk(
                4,
                0,
                "æç¤ºå·¥ç¨‹æ˜¯ä¼˜åŒ–å¤§æ¨¡å‹è¡¨ç°çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬è®¾è®¡æœ‰æ•ˆçš„æç¤ºæ¨¡æ¿ã€ä¸Šä¸‹æ–‡ç®¡ç†ç­‰ã€‚",
            ),
            (4, 1): FileChunk(
                4, 1, "è‰¯å¥½çš„æç¤ºè®¾è®¡åŸåˆ™åŒ…æ‹¬ï¼šæ¸…æ™°æ˜ç¡®ã€æä¾›ç¤ºä¾‹ã€ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ç­‰ã€‚"
            ),
            (4, 2): FileChunk(
                4, 2, "Agent ç³»ç»Ÿçš„æç¤ºè®¾è®¡éœ€è¦è€ƒè™‘å·¥å…·è°ƒç”¨çš„ç­–ç•¥æŒ‡å¯¼å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚"
            ),
            (4, 3): FileChunk(
                4, 3, "ç³»ç»Ÿæç¤ºè¯åº”è¯¥æ˜ç¡®å®šä¹‰ Agent çš„è§’è‰²ã€èƒ½åŠ›è¾¹ç•Œå’Œè¡Œä¸ºè§„èŒƒã€‚"
            ),
        }

    def search(self, kb_id: int, query: str) -> List[Dict]:
        """æ¨¡æ‹Ÿè¯­ä¹‰æœç´¢ - åŸºäºå…³é”®è¯åŒ¹é…"""
        query_lower = query.lower()
        results = []

        for (file_id, chunk_idx), chunk in self.chunks.items():
            content_lower = chunk.content.lower()
            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            score = 0
            keywords = [
                "rag",
                "agentic",
                "ä¼˜ç¼ºç‚¹",
                "ä¼˜ç‚¹",
                "ç¼ºç‚¹",
                "llm",
                "æ£€ç´¢",
                "ç”Ÿæˆ",
                "å‘é‡",
                "æœç´¢",
            ]
            for keyword in keywords:
                if keyword in query_lower and keyword in content_lower:
                    score += 1

            if score > 0 or any(word in content_lower for word in query_lower.split()):
                file_info = next(f for f in self.files if f.id == file_id)
                results.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_idx,
                        "filename": file_info.filename,
                        "score": score + 0.5,  # åŸºç¡€åˆ†
                        "preview": chunk.content[:100] + "..."
                        if len(chunk.content) > 100
                        else chunk.content,
                    }
                )

        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰5ä¸ª
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:5]

    def getFilesMeta(self, kb_id: int, file_ids: List[int]) -> List[Dict]:
        """è·å–æ–‡ä»¶å…ƒä¿¡æ¯"""
        result = []
        for file_id in file_ids:
            file_info = next((f for f in self.files if f.id == file_id), None)
            if file_info:
                result.append(
                    {
                        "id": file_info.id,
                        "filename": file_info.filename,
                        "chunk_count": file_info.chunk_count,
                        "status": file_info.status,
                    }
                )
        return result

    def readFileChunks(self, kb_id: int, chunks: List[Dict[str, int]]) -> List[Dict]:
        """è¯»å–å…·ä½“çš„æ–‡ä»¶ç‰‡æ®µ"""
        result = []
        for chunk_spec in chunks:
            file_id = chunk_spec.get("fileId")
            chunk_index = chunk_spec.get("chunkIndex")

            chunk = self.chunks.get((file_id, chunk_index))
            if chunk:
                result.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_index,
                        "content": chunk.content,
                        "filename": next(
                            f.filename for f in self.files if f.id == file_id
                        ),
                    }
                )
        return result

    def listFilesPaginated(self, kb_id: int, page: int, page_size: int) -> List[Dict]:
        """åˆ†é¡µåˆ—å‡ºæ–‡ä»¶"""
        start = page * page_size
        end = start + page_size

        files_slice = self.files[start:end]
        return [
            {
                "id": f.id,
                "filename": f.filename,
                "chunk_count": f.chunk_count,
                "status": f.status,
            }
            for f in files_slice
        ]


# åˆå§‹åŒ–æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“æ§åˆ¶å™¨
kb_controller = MockKnowledgeBaseController()
knowledge_base_id = 1  # æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“ID


# å®šä¹‰å››ä¸ªæ ¸å¿ƒå·¥å…·
@tool("query_knowledge_base")
def query_knowledge_base(query: str) -> str:
    """Query a knowledge base with semantic search"""
    results = kb_controller.search(knowledge_base_id, query)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("get_files_meta")
def get_files_meta(fileIds: List[int]) -> str:
    """Get metadata for files in the current knowledge base."""
    if not fileIds:
        return "è¯·æä¾›æ–‡ä»¶IDæ•°ç»„"
    results = kb_controller.getFilesMeta(knowledge_base_id, fileIds)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("read_file_chunks")
def read_file_chunks(chunks: List[Dict[str, int]]) -> str:
    """Read content chunks from specified files in the current knowledge base."""
    if not chunks:
        return "è¯·æä¾›è¦è¯»å–çš„chunkä¿¡æ¯æ•°ç»„"
    results = kb_controller.readFileChunks(knowledge_base_id, chunks)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("list_files")
def list_files(page: int = 0, pageSize: int = 10) -> str:
    """List all files in the current knowledge base. Returns file ID, filename, and chunk count."""
    results = kb_controller.listFilesPaginated(knowledge_base_id, page, pageSize)
    return json.dumps(results, ensure_ascii=False, indent=2)


def create_agentic_rag_system():
    """åˆ›å»º Agentic RAG ç³»ç»Ÿ"""

    # å·¥å…·æ¸…å•
    tools = [query_knowledge_base, get_files_meta, read_file_chunks, list_files]

    # è¡Œä¸ºç­–ç•¥ï¼ˆç³»ç»Ÿæç¤ºï¼‰
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ª Agentic RAG åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹ç­–ç•¥é€æ­¥æ”¶é›†è¯æ®åå›ç­”ï¼š

1. å…ˆç”¨ query_knowledge_base æœç´¢ç›¸å…³å†…å®¹ï¼Œè·å¾—å€™é€‰æ–‡ä»¶å’Œç‰‡æ®µçº¿ç´¢
2. æ ¹æ®æœç´¢ç»“æœï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ–‡ä»¶ï¼Œå¯é€‰æ‹©æ€§ä½¿ç”¨ get_files_meta æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶ä¿¡æ¯
3. ä½¿ç”¨ read_file_chunks ç²¾è¯»æœ€ç›¸å…³çš„2-3ä¸ªç‰‡æ®µå†…å®¹ä½œä¸ºè¯æ®
4. åŸºäºè¯»å–çš„å…·ä½“ç‰‡æ®µå†…å®¹ç»„ç»‡ç­”æ¡ˆ
5. å›ç­”æœ«å°¾ç”¨"å¼•ç”¨ï¼š"æ ¼å¼åˆ—å‡ºå®é™…è¯»å–çš„fileIdå’ŒchunkIndex

é‡è¦åŸåˆ™ï¼š
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºå®é™…è¯»å–çš„ç‰‡æ®µå†…å®¹å›ç­”
- è‹¥è¯æ®ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶å»ºè®®è¿›ä¸€æ­¥æœç´¢çš„æ–¹å‘
- ä¼˜å…ˆé€‰æ‹©è¯„åˆ†é«˜çš„æœç´¢ç»“æœè¿›è¡Œæ·±å…¥é˜…è¯»
"""

    # æ¨¡å‹ä¸ Agent
    llm = ChatOpenAI(
        # model="gpt-3.5-turbo",  # ä½¿ç”¨ OpenAI é»˜è®¤æ¨¡å‹ä¾¿äºæµ‹è¯•
        temperature=0,
        max_retries=3,
        # å¦‚éœ€ä½¿ç”¨å…¶ä»–APIï¼Œå¯é…ç½® base_url
        base_url="https://api.siliconflow.cn/v1",
        model="THUDM/glm-4-9b-chat",
    )

    agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)
    return agent


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤º Agentic RAG çš„å·¥ä½œæµç¨‹"""
    print("ğŸš€ åˆå§‹åŒ– Agentic RAG ç³»ç»Ÿ...")
    agent = create_agentic_rag_system()

    print("\nğŸ“š æ¨¡æ‹ŸçŸ¥è¯†åº“åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š")
    for file in kb_controller.files:
        print(f"  - {file.filename} ({file.chunk_count} chunks)")

    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”æ¼”ç¤º")
    print("=" * 80)

    # æµ‹è¯•é—®é¢˜
    question = "è¯·åŸºäºçŸ¥è¯†åº“ï¼Œæ¦‚è¿° RAG çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºå¼•ç”¨ã€‚"
    print(f"\nâ“ é—®é¢˜: {question}")
    print("\nğŸ¤” Agent æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹:")
    print("-" * 50)

    # è°ƒç”¨ Agent
    result = agent.invoke({"messages": [("user", question)]})
    print("======")
    final_answer = result["messages"][-1].content
    print(result)


if __name__ == "__main__":
    main()
